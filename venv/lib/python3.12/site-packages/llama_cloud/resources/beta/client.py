# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing
import urllib.parse
from json.decoder import JSONDecodeError

import typing_extensions

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ...core.remove_none_from_dict import remove_none_from_dict
from ...errors.unprocessable_entity_error import UnprocessableEntityError
from ...types.agent_data import AgentData
from ...types.batch_file_status import BatchFileStatus
from ...types.batch_item_list_response import BatchItemListResponse
from ...types.batch_job_cancel_response import BatchJobCancelResponse
from ...types.batch_job_query_response import BatchJobQueryResponse
from ...types.batch_job_response import BatchJobResponse
from ...types.batch_job_status import BatchJobStatus
from ...types.batch_job_status_response import BatchJobStatusResponse
from ...types.batch_job_type import BatchJobType
from ...types.delete_response import DeleteResponse
from ...types.directory_file_query_response import DirectoryFileQueryResponse
from ...types.directory_file_response import DirectoryFileResponse
from ...types.directory_query_response import DirectoryQueryResponse
from ...types.directory_response import DirectoryResponse
from ...types.directory_sync_job_response import DirectorySyncJobResponse
from ...types.file import File
from ...types.file_create import FileCreate
from ...types.file_filter import FileFilter
from ...types.file_query_response import FileQueryResponse
from ...types.filter_operation import FilterOperation
from ...types.http_validation_error import HttpValidationError
from ...types.item_processing_results_response import ItemProcessingResultsResponse
from ...types.llama_parse_parameters import LlamaParseParameters
from ...types.paginated_response_agent_data import PaginatedResponseAgentData
from ...types.paginated_response_aggregate_group import PaginatedResponseAggregateGroup
from ...types.paginated_response_quota_configuration import PaginatedResponseQuotaConfiguration
from ...types.paginated_response_spreadsheet_job import PaginatedResponseSpreadsheetJob
from ...types.parse_configuration import ParseConfiguration
from ...types.parse_configuration_query_response import ParseConfigurationQueryResponse
from ...types.parse_job_query_response import ParseJobQueryResponse
from ...types.parse_job_response import ParseJobResponse
from ...types.parse_job_result_response import ParseJobResultResponse
from ...types.presigned_url import PresignedUrl
from ...types.split_category import SplitCategory
from ...types.split_document_input import SplitDocumentInput
from ...types.split_job_query_response import SplitJobQueryResponse
from ...types.split_job_response import SplitJobResponse
from ...types.split_strategy import SplitStrategy
from ...types.spreadsheet_job import SpreadsheetJob
from ...types.spreadsheet_parsing_config import SpreadsheetParsingConfig
from ...types.spreadsheet_result_type import SpreadsheetResultType
from ...types.src_parse_schemas_workflow_parse_job_config import SrcParseSchemasWorkflowParseJobConfig
from ...types.usage_metric import UsageMetric
from ...types.usage_metric_query_response import UsageMetricQueryResponse
from ...types.webhook_configuration import WebhookConfiguration
from .types.batch_job_create_request_job_config import BatchJobCreateRequestJobConfig

try:
    import pydantic
    if pydantic.__version__.startswith("1."):
        raise ImportError
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class BetaClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def create_agent_data(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        data: typing.Dict[str, typing.Any],
        deployment_name: str,
    ) -> AgentData:
        """
        Create new agent data.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str].

            - data: typing.Dict[str, typing.Any].

            - deployment_name: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_agent_data(
            data={"string": {}},
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"data": data, "deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def aggregate_agent_data_api_v_1_beta_agent_data_aggregate_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        count: typing.Optional[bool] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
        first: typing.Optional[bool] = OMIT,
        group_by: typing.Optional[typing.List[str]] = OMIT,
        offset: typing.Optional[int] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> PaginatedResponseAggregateGroup:
        """
        Aggregate agent data with grouping and optional counting/first item retrieval.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to aggregate data for

            - count: typing.Optional[bool].

            - deployment_name: str. The agent deployment's name to aggregate data for

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].

            - first: typing.Optional[bool].

            - group_by: typing.Optional[typing.List[str]].

            - offset: typing.Optional[int].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.aggregate_agent_data_api_v_1_beta_agent_data_aggregate_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if count is not OMIT:
            _request["count"] = count
        if filter is not OMIT:
            _request["filter"] = filter
        if first is not OMIT:
            _request["first"] = first
        if group_by is not OMIT:
            _request["group_by"] = group_by
        if offset is not OMIT:
            _request["offset"] = offset
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:aggregate"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseAggregateGroup, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_agent_data_by_query_api_v_1_beta_agent_data_delete_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
    ) -> DeleteResponse:
        """
        Bulk delete agent data by query (deployment_name, collection, optional filters).

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to delete from

            - deployment_name: str. The agent deployment's name to delete data for

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_agent_data_by_query_api_v_1_beta_agent_data_delete_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if filter is not OMIT:
            _request["filter"] = filter
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:delete"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DeleteResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def search_agent_data_api_v_1_beta_agent_data_search_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
        include_total: typing.Optional[bool] = OMIT,
        offset: typing.Optional[int] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> PaginatedResponseAgentData:
        """
        Search agent data with filtering, sorting, and pagination.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to search within

            - deployment_name: str. The agent deployment's name to search within

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].

            - include_total: typing.Optional[bool]. Whether to include the total number of items in the response

            - offset: typing.Optional[int].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.search_agent_data_api_v_1_beta_agent_data_search_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if filter is not OMIT:
            _request["filter"] = filter
        if include_total is not OMIT:
            _request["include_total"] = include_total
        if offset is not OMIT:
            _request["offset"] = offset
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:search"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseAgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_agent_data(
        self, item_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> AgentData:
        """
        Get agent data by ID.

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_agent_data(
            item_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_agent_data(
        self,
        item_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        data: typing.Dict[str, typing.Any],
    ) -> AgentData:
        """
        Update agent data by ID (overwrites).

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - data: typing.Dict[str, typing.Any].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.update_agent_data(
            item_id="string",
            data={"string": {}},
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder({"data": data}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_agent_data(
        self, item_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> typing.Dict[str, str]:
        """
        Delete agent data by ID.

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_agent_data(
            item_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Dict[str, str], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_batch_jobs(
        self,
        *,
        directory_id: typing.Optional[str] = None,
        job_type: typing.Optional[BatchJobType] = None,
        status: typing.Optional[BatchJobStatus] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> BatchJobQueryResponse:
        """
        List all batch processing jobs for a project with optional filtering.

        Returns a paginated list of batch jobs with filters for directory, job type, and status.
        Useful for viewing job history, monitoring progress, and finding specific jobs.

        Parameters:
            - directory_id: typing.Optional[str]. Filter by directory ID

            - job_type: typing.Optional[BatchJobType]. Filter by job type (PARSE, EXTRACT, CLASSIFY)

            - status: typing.Optional[BatchJobStatus]. Filter by job status (PENDING, RUNNING, COMPLETED, FAILED, CANCELLED)

            - limit: typing.Optional[int]. Maximum number of jobs to return

            - offset: typing.Optional[int]. Number of jobs to skip for pagination

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchJobStatus, BatchJobType
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_batch_jobs(
            job_type=BatchJobType.PARSE,
            status=BatchJobStatus.PENDING,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/batch-processing"),
            params=remove_none_from_dict(
                {
                    "directory_id": directory_id,
                    "job_type": job_type,
                    "status": status,
                    "limit": limit,
                    "offset": offset,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_batch_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        continue_as_new_threshold: typing.Optional[int] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        item_ids: typing.Optional[typing.List[str]] = OMIT,
        job_config: BatchJobCreateRequestJobConfig,
        page_size: typing.Optional[int] = OMIT,
        temporal_namespace: typing.Optional[str] = None,
    ) -> BatchJobResponse:
        """
        Create a new batch processing job for a directory.

        Processes all files in the specified directory according to the job configuration.
        The job runs asynchronously and you can monitor progress using the job status endpoint.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - continue_as_new_threshold: typing.Optional[int].

            - directory_id: typing.Optional[str].

            - item_ids: typing.Optional[typing.List[str]].

            - job_config: BatchJobCreateRequestJobConfig. Job configuration for batch processing. Can be BatchParseJobRecordCreate or ClassifyJob.

            - page_size: typing.Optional[int]. Number of files to fetch per batch from the directory (only used in directory mode)

            - temporal_namespace: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_batch_job()
        """
        _request: typing.Dict[str, typing.Any] = {"job_config": job_config}
        if continue_as_new_threshold is not OMIT:
            _request["continue_as_new_threshold"] = continue_as_new_threshold
        if directory_id is not OMIT:
            _request["directory_id"] = directory_id
        if item_ids is not OMIT:
            _request["item_ids"] = item_ids
        if page_size is not OMIT:
            _request["page_size"] = page_size
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/batch-processing"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=remove_none_from_dict(
                {**self._client_wrapper.get_headers(), "temporal-namespace": temporal_namespace}
            ),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_item_processing_results(
        self,
        item_id: str,
        *,
        job_type: typing.Optional[BatchJobType] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ItemProcessingResultsResponse:
        """
        Get all processing results for a specific item (lineage query).

        Shows complete processing history including what operations have been performed,
        with what parameters, and where outputs are stored. Useful for understanding
        what processing has already been done to avoid redundant work. Optionally filter
        by job type to see only specific processing operations.

        Parameters:
            - item_id: str.

            - job_type: typing.Optional[BatchJobType]. Filter results by job type

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchJobType
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_item_processing_results(
            item_id="string",
            job_type=BatchJobType.PARSE,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/batch-processing/items/{item_id}/processing-results",
            ),
            params=remove_none_from_dict(
                {"job_type": job_type, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ItemProcessingResultsResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_batch_job_status(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> BatchJobStatusResponse:
        """
        Get detailed status of a batch processing job.

        Returns current progress, file counts, and estimated completion time.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_batch_job_status(
            job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def cancel_batch_job(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        reason: typing.Optional[str] = OMIT,
        temporal_namespace: typing.Optional[str] = None,
    ) -> BatchJobCancelResponse:
        """
        Cancel a running batch processing job.

        Stops processing and marks all pending items as cancelled. Items currently
        being processed may complete depending on their state.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - reason: typing.Optional[str].

            - temporal_namespace: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.cancel_batch_job(
            job_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if reason is not OMIT:
            _request["reason"] = reason
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}/cancel"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=remove_none_from_dict(
                {**self._client_wrapper.get_headers(), "temporal-namespace": temporal_namespace}
            ),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobCancelResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_batch_job_items(
        self,
        job_id: str,
        *,
        status: typing.Optional[BatchFileStatus] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> BatchItemListResponse:
        """
        List items in a batch job with optional status filtering.

        Useful for finding failed items, viewing completed items, or debugging issues.
        Results are paginated for performance with configurable limit and offset parameters.

        Parameters:
            - job_id: str.

            - status: typing.Optional[BatchFileStatus]. Filter items by status

            - limit: typing.Optional[int]. Maximum number of items to return

            - offset: typing.Optional[int]. Number of items to skip

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchFileStatus
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_batch_job_items(
            job_id="string",
            status=BatchFileStatus.PENDING,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}/items"
            ),
            params=remove_none_from_dict(
                {
                    "status": status,
                    "limit": limit,
                    "offset": offset,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchItemListResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_directories(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        data_source_id: typing.Optional[str] = None,
        include_deleted: typing.Optional[bool] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> DirectoryQueryResponse:
        """
        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - name: typing.Optional[str].

            - data_source_id: typing.Optional[str].

            - include_deleted: typing.Optional[bool].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_directories()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directories"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "name": name,
                    "data_source_id": data_source_id,
                    "include_deleted": include_deleted,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_directory(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        data_source_id: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        name: str,
    ) -> DirectoryResponse:
        """
        Create a new directory within the specified project.

        If data_source_id is provided, validates that the data source exists
        and belongs to the same project.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - data_source_id: typing.Optional[str].

            - description: typing.Optional[str].

            - name: str. Human-readable name for the directory.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_directory(
            name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"name": name}
        if data_source_id is not OMIT:
            _request["data_source_id"] = data_source_id
        if description is not OMIT:
            _request["description"] = description
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directories"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> DirectoryResponse:
        """
        Retrieve a directory by its identifier.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_directory(
            directory_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> None:
        """
        Permanently delete a directory.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_directory(
            directory_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        description: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
    ) -> DirectoryResponse:
        """
        Update directory metadata.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - description: typing.Optional[str].

            - name: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.update_directory(
            directory_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if description is not OMIT:
            _request["description"] = description
        if name is not OMIT:
            _request["name"] = name
        _response = self._client_wrapper.httpx_client.request(
            "PATCH",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_directory_files(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = None,
        display_name_contains: typing.Optional[str] = None,
        unique_id: typing.Optional[str] = None,
        file_id: typing.Optional[str] = None,
        include_deleted: typing.Optional[bool] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> DirectoryFileQueryResponse:
        """
        List all files within the specified directory with optional filtering and pagination.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - display_name_contains: typing.Optional[str].

            - unique_id: typing.Optional[str].

            - file_id: typing.Optional[str].

            - include_deleted: typing.Optional[bool].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_directory_files(
            directory_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files"
            ),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "display_name": display_name,
                    "display_name_contains": display_name_contains,
                    "unique_id": unique_id,
                    "file_id": file_id,
                    "include_deleted": include_deleted,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def add_directory_file(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = OMIT,
        file_id: str,
        unique_id: typing.Optional[str] = OMIT,
    ) -> DirectoryFileResponse:
        """
        Create a new file within the specified directory.

        The directory must exist and belong to the project passed in.
        The file_id must be provided and exist in the project.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - file_id: str. File ID for the storage location (required).

            - unique_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.add_directory_file(
            directory_id="string",
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"file_id": file_id}
        if display_name is not OMIT:
            _request["display_name"] = display_name
        if unique_id is not OMIT:
            _request["unique_id"] = unique_id
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upload_file_to_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = None,
        external_file_id: typing.Optional[str] = None,
        unique_id: typing.Optional[str] = None,
        upload_file: typing.IO,
    ) -> DirectoryFileResponse:
        """
        Upload a file directly to a directory.

        Uploads a file and creates a directory file entry in a single operation.
        If unique_id or display_name are not provided, they will be derived from the file metadata.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - external_file_id: typing.Optional[str].

            - unique_id: typing.Optional[str].

            - upload_file: typing.IO.
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files/upload"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            data=jsonable_encoder(
                {"display_name": display_name, "external_file_id": external_file_id, "unique_id": unique_id}
            ),
            files={"upload_file": upload_file},
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> DirectoryFileResponse:
        """
        Get a file by its directory_file_id within the specified directory. If you're trying to get a file by its unique_id, use the list endpoint with a filter instead.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> None:
        """
        Delete a file from the specified directory.

        Note: This endpoint uses directory_file_id (the internal ID). If you're trying to delete a file by its unique_id, use the list endpoint with a filter to find the directory_file_id first.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = OMIT,
        unique_id: typing.Optional[str] = OMIT,
    ) -> DirectoryFileResponse:
        """
        Update file metadata within the specified directory.

        Note: This endpoint uses directory_file_id (the internal ID). If you're trying to update a file by its unique_id, use the list endpoint with a filter to find the directory_file_id first.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - unique_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.update_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if display_name is not OMIT:
            _request["display_name"] = display_name
        if unique_id is not OMIT:
            _request["unique_id"] = unique_id
        _response = self._client_wrapper.httpx_client.request(
            "PATCH",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_directory_sync_job_api_v_1_beta_directory_sync_jobs_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        directory_id: str,
    ) -> DirectorySyncJobResponse:
        """
        Create a new directory sync job.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - directory_id: str. Directory being processed
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_directory_sync_job_api_v_1_beta_directory_sync_jobs_post(
            directory_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directory-sync-jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder({"directory_id": directory_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectorySyncJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_directory_sync_job(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> DirectorySyncJobResponse:
        """
        Get a directory sync job by ID.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_directory_sync_job(
            job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directory-sync-jobs/{job_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectorySyncJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_file(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: FileCreate,
    ) -> File:
        """
        Create a new file in the project.

        Args:
        file_create: File creation data
        project: Validated project from dependency
        db: Database session

        Returns:
        The created file

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: FileCreate.
        ---
        from llama_cloud import FileCreate
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_file(
            request=FileCreate(
                name="string",
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(File, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def upsert_file(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: FileCreate,
    ) -> File:
        """
        Upsert a file (create or update if exists) in the project.

        Args:
        file_create: File creation/update data
        project: Validated project from dependency
        db: Database session

        Returns:
        The upserted file

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: FileCreate.
        ---
        from llama_cloud import FileCreate
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.upsert_file(
            request=FileCreate(
                name="string",
            ),
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(File, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def query_files(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        filter: typing.Optional[FileFilter] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> FileQueryResponse:
        """
        Query files with flexible filtering and pagination.

        Args:
        request: The query request with filters and pagination
        project: Validated project from dependency
        db: Database session

        Returns:
        Paginated response with files

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - filter: typing.Optional[FileFilter].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud import FileFilter
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.query_files(
            filter=FileFilter(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if filter is not OMIT:
            _request["filter"] = filter
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files/query"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(FileQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_file(
        self, file_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> None:
        """
        Delete a single file from the project.

        Args:
        file_id: The ID of the file to delete
        project: Validated project from dependency
        db: Database session

        Returns:
        None (204 No Content on success)

        Parameters:
            - file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_file(
            file_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/files/{file_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_parse_configurations(
        self,
        *,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        creator: typing.Optional[str] = None,
        version: typing.Optional[str] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ParseConfigurationQueryResponse:
        """
        List parse configurations for the current project.

        Args:
        project: Validated project from dependency
        user: Current user
        db: Database session
        page_size: Number of items per page
        page_token: Token for pagination
        name: Filter by configuration name
        creator: Filter by creator
        version: Filter by version

        Returns:
        Paginated response with parse configurations

        Parameters:
            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].

            - name: typing.Optional[str].

            - creator: typing.Optional[str].

            - version: typing.Optional[str].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_parse_configurations()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parse-configurations"),
            params=remove_none_from_dict(
                {
                    "page_size": page_size,
                    "page_token": page_token,
                    "name": name,
                    "creator": creator,
                    "version": version,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfigurationQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_parse_configuration(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        creator: typing.Optional[str] = OMIT,
        name: str,
        parameters: LlamaParseParameters,
        source_id: typing.Optional[str] = OMIT,
        source_type: typing.Optional[str] = OMIT,
        version: str,
    ) -> ParseConfiguration:
        """
        Create a new parse configuration.

        Args:
        config_create: Parse configuration creation data
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The created parse configuration

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - creator: typing.Optional[str].

            - name: str. Name of the parse configuration

            - parameters: LlamaParseParameters. LlamaParseParameters configuration

            - source_id: typing.Optional[str].

            - source_type: typing.Optional[str].

            - version: str. Version of the configuration
        ---
        from llama_cloud import (
            FailPageMode,
            LlamaParseParameters,
            LlamaParseParametersPriority,
            ParsingMode,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_parse_configuration(
            name="string",
            parameters=LlamaParseParameters(
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=LlamaParseParametersPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
            ),
            version="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"name": name, "parameters": parameters, "version": version}
        if creator is not OMIT:
            _request["creator"] = creator
        if source_id is not OMIT:
            _request["source_id"] = source_id
        if source_type is not OMIT:
            _request["source_type"] = source_type
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parse-configurations"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_parse_configuration(
        self, config_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> ParseConfiguration:
        """
        Get a parse configuration by ID.

        Args:
        config_id: The ID of the parse configuration
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The parse configuration

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_parse_configuration(
            config_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update_parse_configuration(
        self,
        config_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        parameters: typing.Optional[LlamaParseParameters] = OMIT,
    ) -> ParseConfiguration:
        """
        Update a parse configuration.

        Args:
        config_id: The ID of the parse configuration to update
        config_update: Update data
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The updated parse configuration

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - parameters: typing.Optional[LlamaParseParameters].
        ---
        from llama_cloud import (
            FailPageMode,
            LlamaParseParameters,
            LlamaParseParametersPriority,
            ParsingMode,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.update_parse_configuration(
            config_id="string",
            parameters=LlamaParseParameters(
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=LlamaParseParametersPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if parameters is not OMIT:
            _request["parameters"] = parameters
        _response = self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_parse_configuration(
        self, config_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> None:
        """
        Delete a parse configuration.

        Args:
        config_id: The ID of the parse configuration to delete
        project: Validated project from dependency
        user: Current user
        db: Database session

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_parse_configuration(
            config_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_parse_jobs(
        self,
        *,
        status: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ParseJobQueryResponse:
        """
        List parse jobs with optional filtering and pagination.

        Parameters:
            - status: typing.Optional[str]. Filter by job status (pending, running, completed, failed, cancelled)

            - page_size: typing.Optional[int]. Maximum number of jobs to return

            - page_token: typing.Optional[str]. Page token for cursor-based pagination

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_parse_jobs()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parsing-temporal"),
            params=remove_none_from_dict(
                {
                    "status": status,
                    "page_size": page_size,
                    "page_token": page_token,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_parse_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        correlation_id: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        job_name: typing_extensions.Literal["parse_raw_file_job"],
        parameters: typing.Optional[SrcParseSchemasWorkflowParseJobConfig] = OMIT,
        parent_job_execution_id: typing.Optional[str] = OMIT,
        partitions: typing.Dict[str, str],
        parse_job_record_create_project_id: typing.Optional[str] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        user_id: typing.Optional[str] = OMIT,
        webhook_configurations: typing.Optional[typing.List[WebhookConfiguration]] = OMIT,
    ) -> ParseJobResponse:
        """
        Create a new parse job.

        Processes a document asynchronously. Use the job ID to check status and retrieve results.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - correlation_id: typing.Optional[str].

            - created_at: typing.Optional[dt.datetime]. Creation datetime

            - job_name: typing_extensions.Literal["parse_raw_file_job"].

            - parameters: typing.Optional[SrcParseSchemasWorkflowParseJobConfig].

            - parent_job_execution_id: typing.Optional[str].

            - partitions: typing.Dict[str, str]. The partitions for this execution. Used for determining where to save job output.

            - parse_job_record_create_project_id: typing.Optional[str].

            - session_id: typing.Optional[str].

            - user_id: typing.Optional[str].

            - webhook_configurations: typing.Optional[typing.List[WebhookConfiguration]].
        ---
        from llama_cloud import (
            FailPageMode,
            ParsingMode,
            SrcParseSchemasWorkflowParseJobConfig,
            SrcParseSchemasWorkflowParseJobConfigPriority,
        )
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_parse_job(
            job_name="parse_raw_file_job",
            parameters=SrcParseSchemasWorkflowParseJobConfig(
                file_key="string",
                file_name="string",
                lang="string",
                original_file_name="string",
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=SrcParseSchemasWorkflowParseJobConfigPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
                type="parse",
            ),
            partitions={"string": "string"},
        )
        """
        _request: typing.Dict[str, typing.Any] = {"job_name": job_name, "partitions": partitions}
        if correlation_id is not OMIT:
            _request["correlation_id"] = correlation_id
        if created_at is not OMIT:
            _request["created_at"] = created_at
        if parameters is not OMIT:
            _request["parameters"] = parameters
        if parent_job_execution_id is not OMIT:
            _request["parent_job_execution_id"] = parent_job_execution_id
        if parse_job_record_create_project_id is not OMIT:
            _request["project_id"] = parse_job_record_create_project_id
        if session_id is not OMIT:
            _request["session_id"] = session_id
        if user_id is not OMIT:
            _request["user_id"] = user_id
        if webhook_configurations is not OMIT:
            _request["webhook_configurations"] = webhook_configurations
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parsing-temporal"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_parse_job(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> ParseJobResponse:
        """
        Get a parse job by ID.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_parse_job(
            job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parsing-temporal/{job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_parse_job_results(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> typing.Optional[ParseJobResultResponse]:
        """
        Get results for a parse job.

        Returns a single result object containing the full OutputFiles JSON with all output types
        (full_text, md, json_output, job_info, xlsx, output_pdf, structured, images, charts).

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_parse_job_results(
            job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parsing-temporal/{job_id}/results"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Optional[ParseJobResultResponse], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def stream_parse_job_result_content(
        self,
        job_id: str,
        result_type: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.Any:
        """
        Stream parse result content from storage.

        The result_type should be one of: fullText, md, json, jobInfo, xlsx, outputPDF, structured.
        For images and charts, use the index-based approach (e.g., images[0], charts[1]).

        Parameters:
            - job_id: str.

            - result_type: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.stream_parse_job_result_content(
            job_id="string",
            result_type="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/parsing-temporal/{job_id}/results/{result_type}/content",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_quota_configurations(
        self,
        *,
        source_type: typing_extensions.Literal["organization"],
        source_id: str,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
    ) -> PaginatedResponseQuotaConfiguration:
        """
        Retrieve a paginated list of quota configurations with optional filtering.

        Parameters:
            - source_type: typing_extensions.Literal["organization"].

            - source_id: str.

            - page: typing.Optional[int].

            - page_size: typing.Optional[int].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_quota_configurations(
            source_type="organization",
            source_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/quota-management"),
            params=remove_none_from_dict(
                {"source_type": source_type, "source_id": source_id, "page": page, "page_size": page_size}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseQuotaConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_spreadsheet_jobs(
        self,
        *,
        include_results: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> PaginatedResponseSpreadsheetJob:
        """
        List spreadsheet parsing jobs.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - include_results: typing.Optional[bool].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_spreadsheet_jobs()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/sheets/jobs"),
            params=remove_none_from_dict(
                {
                    "include_results": include_results,
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseSpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_spreadsheet_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        config: typing.Optional[SpreadsheetParsingConfig] = OMIT,
        file_id: str,
    ) -> SpreadsheetJob:
        """
        Create a spreadsheet parsing job.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - config: typing.Optional[SpreadsheetParsingConfig]. Configuration for the parsing job

            - file_id: str. The ID of the file to parse
        ---
        from llama_cloud import SpreadsheetParsingConfig
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_spreadsheet_job(
            config=SpreadsheetParsingConfig(),
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"file_id": file_id}
        if config is not OMIT:
            _request["config"] = config
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/sheets/jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_spreadsheet_job(
        self,
        spreadsheet_job_id: str,
        *,
        include_results: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> SpreadsheetJob:
        """
        Get a spreadsheet parsing job.

        When include_results=True (default), the response will include extracted regions and results
        if the job is complete, eliminating the need for a separate /results call.

        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - include_results: typing.Optional[bool].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_spreadsheet_job(
            spreadsheet_job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}"
            ),
            params=remove_none_from_dict(
                {"include_results": include_results, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete_spreadsheet_job(
        self,
        spreadsheet_job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.Any:
        """
        Delete a spreadsheet parsing job and its associated data.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.delete_spreadsheet_job(
            spreadsheet_job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_result_region(
        self,
        spreadsheet_job_id: str,
        region_id: str,
        region_type: SpreadsheetResultType,
        *,
        expires_at_seconds: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> PresignedUrl:
        """
        Generate a presigned URL to download a specific extracted region.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - region_id: str.

            - region_type: SpreadsheetResultType.

            - expires_at_seconds: typing.Optional[int].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import SpreadsheetResultType
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_result_region(
            spreadsheet_job_id="string",
            region_id="string",
            region_type=SpreadsheetResultType.TABLE,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}/regions/{region_id}/result/{region_type}",
            ),
            params=remove_none_from_dict(
                {"expires_at_seconds": expires_at_seconds, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PresignedUrl, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_split_jobs(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> SplitJobQueryResponse:
        """
        List document split jobs.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_split_jobs()
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/split/jobs"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_split_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        categories: typing.List[SplitCategory],
        document_input: SplitDocumentInput,
        splitting_strategy: typing.Optional[SplitStrategy] = OMIT,
    ) -> SplitJobResponse:
        """
        Create a document split job.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - categories: typing.List[SplitCategory]. Categories to split the document into.

            - document_input: SplitDocumentInput. Document to be split.

            - splitting_strategy: typing.Optional[SplitStrategy]. Strategy for splitting the document.
        ---
        from llama_cloud import SplitDocumentInput, SplitStrategy
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.create_split_job(
            categories=[],
            document_input=SplitDocumentInput(
                type="string",
                value="string",
            ),
            splitting_strategy=SplitStrategy(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"categories": categories, "document_input": document_input}
        if splitting_strategy is not OMIT:
            _request["splitting_strategy"] = splitting_strategy
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/split/jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_split_job(
        self,
        split_job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> SplitJobResponse:
        """
        Get a document split job.

        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - split_job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_split_job(
            split_job_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/split/jobs/{split_job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def list_usage_metrics(
        self,
        *,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        include_total: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        user_id: typing.Optional[str] = None,
        event_types: typing.Optional[typing.Union[str, typing.List[str]]] = None,
        days: typing.Optional[typing.Union[str, typing.List[str]]] = None,
        day_on_or_before: typing.Optional[str] = None,
        day_on_or_after: typing.Optional[str] = None,
        event_aggregation_type: typing.Optional[str] = None,
        event_aggregation_key: typing.Optional[str] = None,
        organization_id: str,
    ) -> UsageMetricQueryResponse:
        """
        List usage metrics with filtering and pagination.

        Parameters:
            - page_size: typing.Optional[int]. Number of items per page

            - page_token: typing.Optional[str]. Token for pagination

            - include_total: typing.Optional[bool]. Include total count in response

            - project_id: typing.Optional[str]. Filter by project ID

            - user_id: typing.Optional[str]. Filter by user ID

            - event_types: typing.Optional[typing.Union[str, typing.List[str]]]. Filter by event types

            - days: typing.Optional[typing.Union[str, typing.List[str]]]. Filter by specific days (YYYY-MM-DD)

            - day_on_or_before: typing.Optional[str]. Filter by days on or before this date (YYYY-MM-DD)

            - day_on_or_after: typing.Optional[str]. Filter by days on or after this date (YYYY-MM-DD)

            - event_aggregation_type: typing.Optional[str]. Filter by event aggregation type

            - event_aggregation_key: typing.Optional[str]. Filter by event aggregation key

            - organization_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.list_usage_metrics(
            organization_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/usage-metrics"),
            params=remove_none_from_dict(
                {
                    "page_size": page_size,
                    "page_token": page_token,
                    "include_total": include_total,
                    "project_id": project_id,
                    "user_id": user_id,
                    "event_types": event_types,
                    "days": days,
                    "day_on_or_before": day_on_or_before,
                    "day_on_or_after": day_on_or_after,
                    "event_aggregation_type": event_aggregation_type,
                    "event_aggregation_key": event_aggregation_key,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(UsageMetricQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get_usage_metric(self, usage_metric_id: str) -> UsageMetric:
        """
        Get a usage metric by ID.

        Parameters:
            - usage_metric_id: str.
        ---
        from llama_cloud.client import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
        )
        client.beta.get_usage_metric(
            usage_metric_id="string",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/usage-metrics/{usage_metric_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(UsageMetric, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncBetaClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def create_agent_data(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        data: typing.Dict[str, typing.Any],
        deployment_name: str,
    ) -> AgentData:
        """
        Create new agent data.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str].

            - data: typing.Dict[str, typing.Any].

            - deployment_name: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_agent_data(
            data={"string": {}},
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"data": data, "deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def aggregate_agent_data_api_v_1_beta_agent_data_aggregate_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        count: typing.Optional[bool] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
        first: typing.Optional[bool] = OMIT,
        group_by: typing.Optional[typing.List[str]] = OMIT,
        offset: typing.Optional[int] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> PaginatedResponseAggregateGroup:
        """
        Aggregate agent data with grouping and optional counting/first item retrieval.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to aggregate data for

            - count: typing.Optional[bool].

            - deployment_name: str. The agent deployment's name to aggregate data for

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].

            - first: typing.Optional[bool].

            - group_by: typing.Optional[typing.List[str]].

            - offset: typing.Optional[int].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.aggregate_agent_data_api_v_1_beta_agent_data_aggregate_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if count is not OMIT:
            _request["count"] = count
        if filter is not OMIT:
            _request["filter"] = filter
        if first is not OMIT:
            _request["first"] = first
        if group_by is not OMIT:
            _request["group_by"] = group_by
        if offset is not OMIT:
            _request["offset"] = offset
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:aggregate"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseAggregateGroup, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_agent_data_by_query_api_v_1_beta_agent_data_delete_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
    ) -> DeleteResponse:
        """
        Bulk delete agent data by query (deployment_name, collection, optional filters).

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to delete from

            - deployment_name: str. The agent deployment's name to delete data for

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_agent_data_by_query_api_v_1_beta_agent_data_delete_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if filter is not OMIT:
            _request["filter"] = filter
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:delete"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DeleteResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def search_agent_data_api_v_1_beta_agent_data_search_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        collection: typing.Optional[str] = OMIT,
        deployment_name: str,
        filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]] = OMIT,
        include_total: typing.Optional[bool] = OMIT,
        offset: typing.Optional[int] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> PaginatedResponseAgentData:
        """
        Search agent data with filtering, sorting, and pagination.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - collection: typing.Optional[str]. The logical agent data collection to search within

            - deployment_name: str. The agent deployment's name to search within

            - filter: typing.Optional[typing.Dict[str, typing.Optional[FilterOperation]]].

            - include_total: typing.Optional[bool]. Whether to include the total number of items in the response

            - offset: typing.Optional[int].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.search_agent_data_api_v_1_beta_agent_data_search_post(
            deployment_name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"deployment_name": deployment_name}
        if collection is not OMIT:
            _request["collection"] = collection
        if filter is not OMIT:
            _request["filter"] = filter
        if include_total is not OMIT:
            _request["include_total"] = include_total
        if offset is not OMIT:
            _request["offset"] = offset
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/agent-data/:search"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseAgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_agent_data(
        self, item_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> AgentData:
        """
        Get agent data by ID.

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_agent_data(
            item_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_agent_data(
        self,
        item_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        data: typing.Dict[str, typing.Any],
    ) -> AgentData:
        """
        Update agent data by ID (overwrites).

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - data: typing.Dict[str, typing.Any].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.update_agent_data(
            item_id="string",
            data={"string": {}},
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder({"data": data}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(AgentData, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_agent_data(
        self, item_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> typing.Dict[str, str]:
        """
        Delete agent data by ID.

        Parameters:
            - item_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_agent_data(
            item_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/agent-data/{item_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Dict[str, str], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_batch_jobs(
        self,
        *,
        directory_id: typing.Optional[str] = None,
        job_type: typing.Optional[BatchJobType] = None,
        status: typing.Optional[BatchJobStatus] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> BatchJobQueryResponse:
        """
        List all batch processing jobs for a project with optional filtering.

        Returns a paginated list of batch jobs with filters for directory, job type, and status.
        Useful for viewing job history, monitoring progress, and finding specific jobs.

        Parameters:
            - directory_id: typing.Optional[str]. Filter by directory ID

            - job_type: typing.Optional[BatchJobType]. Filter by job type (PARSE, EXTRACT, CLASSIFY)

            - status: typing.Optional[BatchJobStatus]. Filter by job status (PENDING, RUNNING, COMPLETED, FAILED, CANCELLED)

            - limit: typing.Optional[int]. Maximum number of jobs to return

            - offset: typing.Optional[int]. Number of jobs to skip for pagination

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchJobStatus, BatchJobType
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_batch_jobs(
            job_type=BatchJobType.PARSE,
            status=BatchJobStatus.PENDING,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/batch-processing"),
            params=remove_none_from_dict(
                {
                    "directory_id": directory_id,
                    "job_type": job_type,
                    "status": status,
                    "limit": limit,
                    "offset": offset,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_batch_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        continue_as_new_threshold: typing.Optional[int] = OMIT,
        directory_id: typing.Optional[str] = OMIT,
        item_ids: typing.Optional[typing.List[str]] = OMIT,
        job_config: BatchJobCreateRequestJobConfig,
        page_size: typing.Optional[int] = OMIT,
        temporal_namespace: typing.Optional[str] = None,
    ) -> BatchJobResponse:
        """
        Create a new batch processing job for a directory.

        Processes all files in the specified directory according to the job configuration.
        The job runs asynchronously and you can monitor progress using the job status endpoint.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - continue_as_new_threshold: typing.Optional[int].

            - directory_id: typing.Optional[str].

            - item_ids: typing.Optional[typing.List[str]].

            - job_config: BatchJobCreateRequestJobConfig. Job configuration for batch processing. Can be BatchParseJobRecordCreate or ClassifyJob.

            - page_size: typing.Optional[int]. Number of files to fetch per batch from the directory (only used in directory mode)

            - temporal_namespace: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_batch_job()
        """
        _request: typing.Dict[str, typing.Any] = {"job_config": job_config}
        if continue_as_new_threshold is not OMIT:
            _request["continue_as_new_threshold"] = continue_as_new_threshold
        if directory_id is not OMIT:
            _request["directory_id"] = directory_id
        if item_ids is not OMIT:
            _request["item_ids"] = item_ids
        if page_size is not OMIT:
            _request["page_size"] = page_size
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/batch-processing"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=remove_none_from_dict(
                {**self._client_wrapper.get_headers(), "temporal-namespace": temporal_namespace}
            ),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_item_processing_results(
        self,
        item_id: str,
        *,
        job_type: typing.Optional[BatchJobType] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ItemProcessingResultsResponse:
        """
        Get all processing results for a specific item (lineage query).

        Shows complete processing history including what operations have been performed,
        with what parameters, and where outputs are stored. Useful for understanding
        what processing has already been done to avoid redundant work. Optionally filter
        by job type to see only specific processing operations.

        Parameters:
            - item_id: str.

            - job_type: typing.Optional[BatchJobType]. Filter results by job type

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchJobType
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_item_processing_results(
            item_id="string",
            job_type=BatchJobType.PARSE,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/batch-processing/items/{item_id}/processing-results",
            ),
            params=remove_none_from_dict(
                {"job_type": job_type, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ItemProcessingResultsResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_batch_job_status(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> BatchJobStatusResponse:
        """
        Get detailed status of a batch processing job.

        Returns current progress, file counts, and estimated completion time.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_batch_job_status(
            job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobStatusResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def cancel_batch_job(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        reason: typing.Optional[str] = OMIT,
        temporal_namespace: typing.Optional[str] = None,
    ) -> BatchJobCancelResponse:
        """
        Cancel a running batch processing job.

        Stops processing and marks all pending items as cancelled. Items currently
        being processed may complete depending on their state.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - reason: typing.Optional[str].

            - temporal_namespace: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.cancel_batch_job(
            job_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if reason is not OMIT:
            _request["reason"] = reason
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}/cancel"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=remove_none_from_dict(
                {**self._client_wrapper.get_headers(), "temporal-namespace": temporal_namespace}
            ),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchJobCancelResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_batch_job_items(
        self,
        job_id: str,
        *,
        status: typing.Optional[BatchFileStatus] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> BatchItemListResponse:
        """
        List items in a batch job with optional status filtering.

        Useful for finding failed items, viewing completed items, or debugging issues.
        Results are paginated for performance with configurable limit and offset parameters.

        Parameters:
            - job_id: str.

            - status: typing.Optional[BatchFileStatus]. Filter items by status

            - limit: typing.Optional[int]. Maximum number of items to return

            - offset: typing.Optional[int]. Number of items to skip

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import BatchFileStatus
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_batch_job_items(
            job_id="string",
            status=BatchFileStatus.PENDING,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/batch-processing/{job_id}/items"
            ),
            params=remove_none_from_dict(
                {
                    "status": status,
                    "limit": limit,
                    "offset": offset,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(BatchItemListResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_directories(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        data_source_id: typing.Optional[str] = None,
        include_deleted: typing.Optional[bool] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> DirectoryQueryResponse:
        """
        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - name: typing.Optional[str].

            - data_source_id: typing.Optional[str].

            - include_deleted: typing.Optional[bool].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_directories()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directories"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "name": name,
                    "data_source_id": data_source_id,
                    "include_deleted": include_deleted,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_directory(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        data_source_id: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        name: str,
    ) -> DirectoryResponse:
        """
        Create a new directory within the specified project.

        If data_source_id is provided, validates that the data source exists
        and belongs to the same project.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - data_source_id: typing.Optional[str].

            - description: typing.Optional[str].

            - name: str. Human-readable name for the directory.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_directory(
            name="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"name": name}
        if data_source_id is not OMIT:
            _request["data_source_id"] = data_source_id
        if description is not OMIT:
            _request["description"] = description
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directories"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> DirectoryResponse:
        """
        Retrieve a directory by its identifier.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_directory(
            directory_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> None:
        """
        Permanently delete a directory.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_directory(
            directory_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        description: typing.Optional[str] = OMIT,
        name: typing.Optional[str] = OMIT,
    ) -> DirectoryResponse:
        """
        Update directory metadata.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - description: typing.Optional[str].

            - name: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.update_directory(
            directory_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if description is not OMIT:
            _request["description"] = description
        if name is not OMIT:
            _request["name"] = name
        _response = await self._client_wrapper.httpx_client.request(
            "PATCH",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_directory_files(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = None,
        display_name_contains: typing.Optional[str] = None,
        unique_id: typing.Optional[str] = None,
        file_id: typing.Optional[str] = None,
        include_deleted: typing.Optional[bool] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> DirectoryFileQueryResponse:
        """
        List all files within the specified directory with optional filtering and pagination.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - display_name_contains: typing.Optional[str].

            - unique_id: typing.Optional[str].

            - file_id: typing.Optional[str].

            - include_deleted: typing.Optional[bool].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_directory_files(
            directory_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files"
            ),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "display_name": display_name,
                    "display_name_contains": display_name_contains,
                    "unique_id": unique_id,
                    "file_id": file_id,
                    "include_deleted": include_deleted,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def add_directory_file(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = OMIT,
        file_id: str,
        unique_id: typing.Optional[str] = OMIT,
    ) -> DirectoryFileResponse:
        """
        Create a new file within the specified directory.

        The directory must exist and belong to the project passed in.
        The file_id must be provided and exist in the project.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - file_id: str. File ID for the storage location (required).

            - unique_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.add_directory_file(
            directory_id="string",
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"file_id": file_id}
        if display_name is not OMIT:
            _request["display_name"] = display_name
        if unique_id is not OMIT:
            _request["unique_id"] = unique_id
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upload_file_to_directory(
        self,
        directory_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = None,
        external_file_id: typing.Optional[str] = None,
        unique_id: typing.Optional[str] = None,
        upload_file: typing.IO,
    ) -> DirectoryFileResponse:
        """
        Upload a file directly to a directory.

        Uploads a file and creates a directory file entry in a single operation.
        If unique_id or display_name are not provided, they will be derived from the file metadata.

        Parameters:
            - directory_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - external_file_id: typing.Optional[str].

            - unique_id: typing.Optional[str].

            - upload_file: typing.IO.
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directories/{directory_id}/files/upload"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            data=jsonable_encoder(
                {"display_name": display_name, "external_file_id": external_file_id, "unique_id": unique_id}
            ),
            files={"upload_file": upload_file},
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> DirectoryFileResponse:
        """
        Get a file by its directory_file_id within the specified directory. If you're trying to get a file by its unique_id, use the list endpoint with a filter instead.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> None:
        """
        Delete a file from the specified directory.

        Note: This endpoint uses directory_file_id (the internal ID). If you're trying to delete a file by its unique_id, use the list endpoint with a filter to find the directory_file_id first.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_directory_file(
        self,
        directory_id: str,
        directory_file_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        display_name: typing.Optional[str] = OMIT,
        unique_id: typing.Optional[str] = OMIT,
    ) -> DirectoryFileResponse:
        """
        Update file metadata within the specified directory.

        Note: This endpoint uses directory_file_id (the internal ID). If you're trying to update a file by its unique_id, use the list endpoint with a filter to find the directory_file_id first.

        Parameters:
            - directory_id: str.

            - directory_file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - display_name: typing.Optional[str].

            - unique_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.update_directory_file(
            directory_id="string",
            directory_file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if display_name is not OMIT:
            _request["display_name"] = display_name
        if unique_id is not OMIT:
            _request["unique_id"] = unique_id
        _response = await self._client_wrapper.httpx_client.request(
            "PATCH",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/directories/{directory_id}/files/{directory_file_id}",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectoryFileResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_directory_sync_job_api_v_1_beta_directory_sync_jobs_post(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        directory_id: str,
    ) -> DirectorySyncJobResponse:
        """
        Create a new directory sync job.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - directory_id: str. Directory being processed
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_directory_sync_job_api_v_1_beta_directory_sync_jobs_post(
            directory_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/directory-sync-jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder({"directory_id": directory_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectorySyncJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_directory_sync_job(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> DirectorySyncJobResponse:
        """
        Get a directory sync job by ID.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_directory_sync_job(
            job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/directory-sync-jobs/{job_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(DirectorySyncJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_file(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: FileCreate,
    ) -> File:
        """
        Create a new file in the project.

        Args:
        file_create: File creation data
        project: Validated project from dependency
        db: Database session

        Returns:
        The created file

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: FileCreate.
        ---
        from llama_cloud import FileCreate
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_file(
            request=FileCreate(
                name="string",
            ),
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(File, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def upsert_file(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request: FileCreate,
    ) -> File:
        """
        Upsert a file (create or update if exists) in the project.

        Args:
        file_create: File creation/update data
        project: Validated project from dependency
        db: Database session

        Returns:
        The upserted file

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - request: FileCreate.
        ---
        from llama_cloud import FileCreate
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.upsert_file(
            request=FileCreate(
                name="string",
            ),
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(File, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def query_files(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        filter: typing.Optional[FileFilter] = OMIT,
        order_by: typing.Optional[str] = OMIT,
        page_size: typing.Optional[int] = OMIT,
        page_token: typing.Optional[str] = OMIT,
    ) -> FileQueryResponse:
        """
        Query files with flexible filtering and pagination.

        Args:
        request: The query request with filters and pagination
        project: Validated project from dependency
        db: Database session

        Returns:
        Paginated response with files

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - filter: typing.Optional[FileFilter].

            - order_by: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud import FileFilter
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.query_files(
            filter=FileFilter(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if filter is not OMIT:
            _request["filter"] = filter
        if order_by is not OMIT:
            _request["order_by"] = order_by
        if page_size is not OMIT:
            _request["page_size"] = page_size
        if page_token is not OMIT:
            _request["page_token"] = page_token
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/files/query"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(FileQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_file(
        self, file_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> None:
        """
        Delete a single file from the project.

        Args:
        file_id: The ID of the file to delete
        project: Validated project from dependency
        db: Database session

        Returns:
        None (204 No Content on success)

        Parameters:
            - file_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_file(
            file_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/files/{file_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_parse_configurations(
        self,
        *,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        creator: typing.Optional[str] = None,
        version: typing.Optional[str] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ParseConfigurationQueryResponse:
        """
        List parse configurations for the current project.

        Args:
        project: Validated project from dependency
        user: Current user
        db: Database session
        page_size: Number of items per page
        page_token: Token for pagination
        name: Filter by configuration name
        creator: Filter by creator
        version: Filter by version

        Returns:
        Paginated response with parse configurations

        Parameters:
            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].

            - name: typing.Optional[str].

            - creator: typing.Optional[str].

            - version: typing.Optional[str].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_parse_configurations()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parse-configurations"),
            params=remove_none_from_dict(
                {
                    "page_size": page_size,
                    "page_token": page_token,
                    "name": name,
                    "creator": creator,
                    "version": version,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfigurationQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_parse_configuration(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        creator: typing.Optional[str] = OMIT,
        name: str,
        parameters: LlamaParseParameters,
        source_id: typing.Optional[str] = OMIT,
        source_type: typing.Optional[str] = OMIT,
        version: str,
    ) -> ParseConfiguration:
        """
        Create a new parse configuration.

        Args:
        config_create: Parse configuration creation data
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The created parse configuration

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - creator: typing.Optional[str].

            - name: str. Name of the parse configuration

            - parameters: LlamaParseParameters. LlamaParseParameters configuration

            - source_id: typing.Optional[str].

            - source_type: typing.Optional[str].

            - version: str. Version of the configuration
        ---
        from llama_cloud import (
            FailPageMode,
            LlamaParseParameters,
            LlamaParseParametersPriority,
            ParsingMode,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_parse_configuration(
            name="string",
            parameters=LlamaParseParameters(
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=LlamaParseParametersPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
            ),
            version="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"name": name, "parameters": parameters, "version": version}
        if creator is not OMIT:
            _request["creator"] = creator
        if source_id is not OMIT:
            _request["source_id"] = source_id
        if source_type is not OMIT:
            _request["source_type"] = source_type
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parse-configurations"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_parse_configuration(
        self, config_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> ParseConfiguration:
        """
        Get a parse configuration by ID.

        Args:
        config_id: The ID of the parse configuration
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The parse configuration

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_parse_configuration(
            config_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update_parse_configuration(
        self,
        config_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        parameters: typing.Optional[LlamaParseParameters] = OMIT,
    ) -> ParseConfiguration:
        """
        Update a parse configuration.

        Args:
        config_id: The ID of the parse configuration to update
        config_update: Update data
        project: Validated project from dependency
        user: Current user
        db: Database session

        Returns:
        The updated parse configuration

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - parameters: typing.Optional[LlamaParseParameters].
        ---
        from llama_cloud import (
            FailPageMode,
            LlamaParseParameters,
            LlamaParseParametersPriority,
            ParsingMode,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.update_parse_configuration(
            config_id="string",
            parameters=LlamaParseParameters(
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=LlamaParseParametersPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
            ),
        )
        """
        _request: typing.Dict[str, typing.Any] = {}
        if parameters is not OMIT:
            _request["parameters"] = parameters
        _response = await self._client_wrapper.httpx_client.request(
            "PUT",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_parse_configuration(
        self, config_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> None:
        """
        Delete a parse configuration.

        Args:
        config_id: The ID of the parse configuration to delete
        project: Validated project from dependency
        user: Current user
        db: Database session

        Parameters:
            - config_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_parse_configuration(
            config_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parse-configurations/{config_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_parse_jobs(
        self,
        *,
        status: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> ParseJobQueryResponse:
        """
        List parse jobs with optional filtering and pagination.

        Parameters:
            - status: typing.Optional[str]. Filter by job status (pending, running, completed, failed, cancelled)

            - page_size: typing.Optional[int]. Maximum number of jobs to return

            - page_token: typing.Optional[str]. Page token for cursor-based pagination

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_parse_jobs()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parsing-temporal"),
            params=remove_none_from_dict(
                {
                    "status": status,
                    "page_size": page_size,
                    "page_token": page_token,
                    "project_id": project_id,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_parse_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        correlation_id: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        job_name: typing_extensions.Literal["parse_raw_file_job"],
        parameters: typing.Optional[SrcParseSchemasWorkflowParseJobConfig] = OMIT,
        parent_job_execution_id: typing.Optional[str] = OMIT,
        partitions: typing.Dict[str, str],
        parse_job_record_create_project_id: typing.Optional[str] = OMIT,
        session_id: typing.Optional[str] = OMIT,
        user_id: typing.Optional[str] = OMIT,
        webhook_configurations: typing.Optional[typing.List[WebhookConfiguration]] = OMIT,
    ) -> ParseJobResponse:
        """
        Create a new parse job.

        Processes a document asynchronously. Use the job ID to check status and retrieve results.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - correlation_id: typing.Optional[str].

            - created_at: typing.Optional[dt.datetime]. Creation datetime

            - job_name: typing_extensions.Literal["parse_raw_file_job"].

            - parameters: typing.Optional[SrcParseSchemasWorkflowParseJobConfig].

            - parent_job_execution_id: typing.Optional[str].

            - partitions: typing.Dict[str, str]. The partitions for this execution. Used for determining where to save job output.

            - parse_job_record_create_project_id: typing.Optional[str].

            - session_id: typing.Optional[str].

            - user_id: typing.Optional[str].

            - webhook_configurations: typing.Optional[typing.List[WebhookConfiguration]].
        ---
        from llama_cloud import (
            FailPageMode,
            ParsingMode,
            SrcParseSchemasWorkflowParseJobConfig,
            SrcParseSchemasWorkflowParseJobConfigPriority,
        )
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_parse_job(
            job_name="parse_raw_file_job",
            parameters=SrcParseSchemasWorkflowParseJobConfig(
                file_key="string",
                file_name="string",
                lang="string",
                original_file_name="string",
                parse_mode=ParsingMode.PARSE_PAGE_WITHOUT_LLM,
                priority=SrcParseSchemasWorkflowParseJobConfigPriority.LOW,
                replace_failed_page_mode=FailPageMode.RAW_TEXT,
                type="parse",
            ),
            partitions={"string": "string"},
        )
        """
        _request: typing.Dict[str, typing.Any] = {"job_name": job_name, "partitions": partitions}
        if correlation_id is not OMIT:
            _request["correlation_id"] = correlation_id
        if created_at is not OMIT:
            _request["created_at"] = created_at
        if parameters is not OMIT:
            _request["parameters"] = parameters
        if parent_job_execution_id is not OMIT:
            _request["parent_job_execution_id"] = parent_job_execution_id
        if parse_job_record_create_project_id is not OMIT:
            _request["project_id"] = parse_job_record_create_project_id
        if session_id is not OMIT:
            _request["session_id"] = session_id
        if user_id is not OMIT:
            _request["user_id"] = user_id
        if webhook_configurations is not OMIT:
            _request["webhook_configurations"] = webhook_configurations
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/parsing-temporal"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_parse_job(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> ParseJobResponse:
        """
        Get a parse job by ID.

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_parse_job(
            job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parsing-temporal/{job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(ParseJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_parse_job_results(
        self, job_id: str, *, project_id: typing.Optional[str] = None, organization_id: typing.Optional[str] = None
    ) -> typing.Optional[ParseJobResultResponse]:
        """
        Get results for a parse job.

        Returns a single result object containing the full OutputFiles JSON with all output types
        (full_text, md, json_output, job_info, xlsx, output_pdf, structured, images, charts).

        Parameters:
            - job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_parse_job_results(
            job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/parsing-temporal/{job_id}/results"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Optional[ParseJobResultResponse], _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def stream_parse_job_result_content(
        self,
        job_id: str,
        result_type: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.Any:
        """
        Stream parse result content from storage.

        The result_type should be one of: fullText, md, json, jobInfo, xlsx, outputPDF, structured.
        For images and charts, use the index-based approach (e.g., images[0], charts[1]).

        Parameters:
            - job_id: str.

            - result_type: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.stream_parse_job_result_content(
            job_id="string",
            result_type="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/parsing-temporal/{job_id}/results/{result_type}/content",
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_quota_configurations(
        self,
        *,
        source_type: typing_extensions.Literal["organization"],
        source_id: str,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
    ) -> PaginatedResponseQuotaConfiguration:
        """
        Retrieve a paginated list of quota configurations with optional filtering.

        Parameters:
            - source_type: typing_extensions.Literal["organization"].

            - source_id: str.

            - page: typing.Optional[int].

            - page_size: typing.Optional[int].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_quota_configurations(
            source_type="organization",
            source_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/quota-management"),
            params=remove_none_from_dict(
                {"source_type": source_type, "source_id": source_id, "page": page, "page_size": page_size}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseQuotaConfiguration, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_spreadsheet_jobs(
        self,
        *,
        include_results: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> PaginatedResponseSpreadsheetJob:
        """
        List spreadsheet parsing jobs.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - include_results: typing.Optional[bool].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_spreadsheet_jobs()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/sheets/jobs"),
            params=remove_none_from_dict(
                {
                    "include_results": include_results,
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PaginatedResponseSpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_spreadsheet_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        config: typing.Optional[SpreadsheetParsingConfig] = OMIT,
        file_id: str,
    ) -> SpreadsheetJob:
        """
        Create a spreadsheet parsing job.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - config: typing.Optional[SpreadsheetParsingConfig]. Configuration for the parsing job

            - file_id: str. The ID of the file to parse
        ---
        from llama_cloud import SpreadsheetParsingConfig
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_spreadsheet_job(
            config=SpreadsheetParsingConfig(),
            file_id="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"file_id": file_id}
        if config is not OMIT:
            _request["config"] = config
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/sheets/jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_spreadsheet_job(
        self,
        spreadsheet_job_id: str,
        *,
        include_results: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> SpreadsheetJob:
        """
        Get a spreadsheet parsing job.

        When include_results=True (default), the response will include extracted regions and results
        if the job is complete, eliminating the need for a separate /results call.

        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - include_results: typing.Optional[bool].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_spreadsheet_job(
            spreadsheet_job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}"
            ),
            params=remove_none_from_dict(
                {"include_results": include_results, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SpreadsheetJob, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete_spreadsheet_job(
        self,
        spreadsheet_job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> typing.Any:
        """
        Delete a spreadsheet parsing job and its associated data.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.delete_spreadsheet_job(
            spreadsheet_job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "DELETE",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}"
            ),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(typing.Any, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_result_region(
        self,
        spreadsheet_job_id: str,
        region_id: str,
        region_type: SpreadsheetResultType,
        *,
        expires_at_seconds: typing.Optional[int] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> PresignedUrl:
        """
        Generate a presigned URL to download a specific extracted region.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - spreadsheet_job_id: str.

            - region_id: str.

            - region_type: SpreadsheetResultType.

            - expires_at_seconds: typing.Optional[int].

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud import SpreadsheetResultType
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_result_region(
            spreadsheet_job_id="string",
            region_id="string",
            region_type=SpreadsheetResultType.TABLE,
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/",
                f"api/v1/beta/sheets/jobs/{spreadsheet_job_id}/regions/{region_id}/result/{region_type}",
            ),
            params=remove_none_from_dict(
                {"expires_at_seconds": expires_at_seconds, "project_id": project_id, "organization_id": organization_id}
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(PresignedUrl, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_split_jobs(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
    ) -> SplitJobQueryResponse:
        """
        List document split jobs.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - page_size: typing.Optional[int].

            - page_token: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_split_jobs()
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/split/jobs"),
            params=remove_none_from_dict(
                {
                    "project_id": project_id,
                    "organization_id": organization_id,
                    "page_size": page_size,
                    "page_token": page_token,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_split_job(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        categories: typing.List[SplitCategory],
        document_input: SplitDocumentInput,
        splitting_strategy: typing.Optional[SplitStrategy] = OMIT,
    ) -> SplitJobResponse:
        """
        Create a document split job.
        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].

            - categories: typing.List[SplitCategory]. Categories to split the document into.

            - document_input: SplitDocumentInput. Document to be split.

            - splitting_strategy: typing.Optional[SplitStrategy]. Strategy for splitting the document.
        ---
        from llama_cloud import SplitDocumentInput, SplitStrategy
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.create_split_job(
            categories=[],
            document_input=SplitDocumentInput(
                type="string",
                value="string",
            ),
            splitting_strategy=SplitStrategy(),
        )
        """
        _request: typing.Dict[str, typing.Any] = {"categories": categories, "document_input": document_input}
        if splitting_strategy is not OMIT:
            _request["splitting_strategy"] = splitting_strategy
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/split/jobs"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_split_job(
        self,
        split_job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
    ) -> SplitJobResponse:
        """
        Get a document split job.

        Experimental: This endpoint is not yet ready for production use and is subject to change at any time.

        Parameters:
            - split_job_id: str.

            - project_id: typing.Optional[str].

            - organization_id: typing.Optional[str].
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_split_job(
            split_job_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/split/jobs/{split_job_id}"),
            params=remove_none_from_dict({"project_id": project_id, "organization_id": organization_id}),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(SplitJobResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def list_usage_metrics(
        self,
        *,
        page_size: typing.Optional[int] = None,
        page_token: typing.Optional[str] = None,
        include_total: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        user_id: typing.Optional[str] = None,
        event_types: typing.Optional[typing.Union[str, typing.List[str]]] = None,
        days: typing.Optional[typing.Union[str, typing.List[str]]] = None,
        day_on_or_before: typing.Optional[str] = None,
        day_on_or_after: typing.Optional[str] = None,
        event_aggregation_type: typing.Optional[str] = None,
        event_aggregation_key: typing.Optional[str] = None,
        organization_id: str,
    ) -> UsageMetricQueryResponse:
        """
        List usage metrics with filtering and pagination.

        Parameters:
            - page_size: typing.Optional[int]. Number of items per page

            - page_token: typing.Optional[str]. Token for pagination

            - include_total: typing.Optional[bool]. Include total count in response

            - project_id: typing.Optional[str]. Filter by project ID

            - user_id: typing.Optional[str]. Filter by user ID

            - event_types: typing.Optional[typing.Union[str, typing.List[str]]]. Filter by event types

            - days: typing.Optional[typing.Union[str, typing.List[str]]]. Filter by specific days (YYYY-MM-DD)

            - day_on_or_before: typing.Optional[str]. Filter by days on or before this date (YYYY-MM-DD)

            - day_on_or_after: typing.Optional[str]. Filter by days on or after this date (YYYY-MM-DD)

            - event_aggregation_type: typing.Optional[str]. Filter by event aggregation type

            - event_aggregation_key: typing.Optional[str]. Filter by event aggregation key

            - organization_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.list_usage_metrics(
            organization_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(f"{self._client_wrapper.get_base_url()}/", "api/v1/beta/usage-metrics"),
            params=remove_none_from_dict(
                {
                    "page_size": page_size,
                    "page_token": page_token,
                    "include_total": include_total,
                    "project_id": project_id,
                    "user_id": user_id,
                    "event_types": event_types,
                    "days": days,
                    "day_on_or_before": day_on_or_before,
                    "day_on_or_after": day_on_or_after,
                    "event_aggregation_type": event_aggregation_type,
                    "event_aggregation_key": event_aggregation_key,
                    "organization_id": organization_id,
                }
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(UsageMetricQueryResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get_usage_metric(self, usage_metric_id: str) -> UsageMetric:
        """
        Get a usage metric by ID.

        Parameters:
            - usage_metric_id: str.
        ---
        from llama_cloud.client import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
        )
        await client.beta.get_usage_metric(
            usage_metric_id="string",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "GET",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_base_url()}/", f"api/v1/beta/usage-metrics/{usage_metric_id}"
            ),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(UsageMetric, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(pydantic.parse_obj_as(HttpValidationError, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
